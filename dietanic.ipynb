{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA To Prediction(DieTanic)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Contents of the Notebook :\n\nPart1 : Exploratory Data Analysis(EDA)\n1. Analysis of the features (특징 분석)\n2. Finding any relations or trends considering multiple features (특성들 간의 관계 찾기)\n\nPart2 : Feature Engineering and Data Cleaning\n1. Adding any few features (몇 가지 특성 추가)\n2. Removing redundant features (중복 특성 제거)\n3. Converting features into suitable form for modeling (특성 변환)\n\nPart3 : Predictive Modeling\n1. Running Basic Algorithms (기본 알고리즘 실행)\n2. Cross Validation (교차 검증)\n3. Ensembling (앙상블)\n4. Important Features Extraction (중요 기능 추출?)","metadata":{}},{"cell_type":"markdown","source":"### Part1: Exploratory Data Analysis(EDA)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight') # stylesheet 종류 및 설정 변경\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-31T14:36:23.485901Z","iopub.execute_input":"2022-01-31T14:36:23.486246Z","iopub.status.idle":"2022-01-31T14:36:23.493375Z","shell.execute_reply.started":"2022-01-31T14:36:23.486202Z","shell.execute_reply":"2022-01-31T14:36:23.492476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T14:36:25.268994Z","iopub.execute_input":"2022-01-31T14:36:25.269453Z","iopub.status.idle":"2022-01-31T14:36:25.299974Z","shell.execute_reply.started":"2022-01-31T14:36:25.269409Z","shell.execute_reply":"2022-01-31T14:36:25.299273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T14:36:25.91394Z","iopub.execute_input":"2022-01-31T14:36:25.914249Z","iopub.status.idle":"2022-01-31T14:36:25.93989Z","shell.execute_reply.started":"2022-01-31T14:36:25.914212Z","shell.execute_reply":"2022-01-31T14:36:25.939266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum() # 총 null 값 체크","metadata":{"execution":{"iopub.status.busy":"2022-01-31T14:36:27.464453Z","iopub.execute_input":"2022-01-31T14:36:27.464883Z","iopub.status.idle":"2022-01-31T14:36:27.475812Z","shell.execute_reply.started":"2022-01-31T14:36:27.464831Z","shell.execute_reply":"2022-01-31T14:36:27.475021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Age, Cabin and Embarked have null values. I will try to fix them.\n나이, 카빈, 항구가 null 값을 가지고 있다. 고쳐보자","metadata":{"execution":{"iopub.status.busy":"2022-01-27T14:41:29.723803Z","iopub.execute_input":"2022-01-27T14:41:29.724124Z","iopub.status.idle":"2022-01-27T14:41:29.73075Z","shell.execute_reply.started":"2022-01-27T14:41:29.72409Z","shell.execute_reply":"2022-01-27T14:41:29.729379Z"}}},{"cell_type":"markdown","source":"#### How many Survived?? 얼마나 많이 살았는가?","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(18,8))                        # %% 이거 빠트리면 안나옴 ... autopct로 파이 안에 들어가는 백분율을 지정\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:43.901705Z","iopub.execute_input":"2022-01-30T10:02:43.901991Z","iopub.status.idle":"2022-01-30T10:02:44.184893Z","shell.execute_reply.started":"2022-01-30T10:02:43.90194Z","shell.execute_reply":"2022-01-30T10:02:44.184006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident that not many passengers survived the accident.\n살아남은 승객이 적다는 것이 명백함\n\nOut of 891 passengers survived the accident. Out of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n승객 891중 350명만이 살아남았다. 우리는 데이터에서 더 나은 인사이트를 얻고 어떤 범주의 승객들이 살아남았는지에 대해 더 깊이 알려고 한다.\n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port of Embarcation, Age, etc.\n데이터셋의 다양한 특성들을 이용해 생존율을 확인하도록 하겠습니다. 특징으로는 성별, 항구, 나이 등등이 있다\n\nFirst let us understand the different types of features.\n먼저 여러가지 특징에 대해 알아보자","metadata":{}},{"cell_type":"markdown","source":"### Types Of Features 특성 타입\n\n#### Categorical Features : 범주형 특성\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them. For example, gender is a categorical variable having two categories(male and female). Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables. \n범주형 특성은 두개 이상의 범주가 있는 변수이며 해당 특성의 값은 각각 분류될 수 있다.예를 들어 성별은 두가지 범주(남성 및 여성)가 있는 특성. 이런 특성은 정렬하거나 순서를 지정할 수 없다. 공칭 변수라고도 한다\n\nCategorical Features in the dataset : Sex, Embarked.\n성별,항구\n\n#### Ordinal Features : 순서형 특성\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg :  if we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort the variable.\n순서형 변수는 범주형과 유사하지만 값 간의 상대적 순서 지정 또는 정렬이 가능하다는 점이 다르다. 높음, 중간, 짧음 으로 나뉜 특성이 있으면 높이는 순서형 변수. 변수를 정렬할 수 있다\n\nOrdinal Features in the dataset : PClass\n\n#### Continous Feature : 연속형 특성\nA feature is said to be continous if can take values between any two points or between the minimum or maximum values in the features column.\n열의 두 점 또는 최소값과 최댓값 사이의 값을 취할 수 있는 경우 연속형\nContinous Features in the dataset : Age 나이\n\n### Analysing The Features\n특성 분석\n### Sex -> Categorical Feature\n성별 -> 범주형 \n\n\n","metadata":{}},{"cell_type":"code","source":"data.groupby(['Sex', 'Survived'])['Survived'].count()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:44.186538Z","iopub.execute_input":"2022-01-30T10:02:44.186901Z","iopub.status.idle":"2022-01-30T10:02:44.1976Z","shell.execute_reply.started":"2022-01-30T10:02:44.186849Z","shell.execute_reply":"2022-01-30T10:02:44.196767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(18,8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:44.19878Z","iopub.execute_input":"2022-01-30T10:02:44.199374Z","iopub.status.idle":"2022-01-30T10:02:44.526888Z","shell.execute_reply.started":"2022-01-30T10:02:44.199283Z","shell.execute_reply":"2022-01-30T10:02:44.526167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This looks interesting. The number of men on the ship is lot more than the number of women.\n배에 타고 있는 남자수가 여자수보다 훨씬 많다\nStill the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%.\n구조된 여성의 수는 남성보다 거의 두배. 배에 탑승한 여성의 생존율은 약 75퍼인 반면 남성은 약 18-19\nThis looks to be a very important feature for modeling. But is it the best?? Lets check other features.\n모델링에 매우 중요한 특징으로 보인다.일단 다른 특성들도 확인해보자\n\n### Pclass -> Ordinal Feature\n피클래스 -> 순서형 특성","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:44.527967Z","iopub.execute_input":"2022-01-30T10:02:44.528182Z","iopub.status.idle":"2022-01-30T10:02:44.574162Z","shell.execute_reply.started":"2022-01-30T10:02:44.528155Z","shell.execute_reply":"2022-01-30T10:02:44.573362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32', '#FFDF00', '#D3D3D3'], ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:44.575709Z","iopub.execute_input":"2022-01-30T10:02:44.576078Z","iopub.status.idle":"2022-01-30T10:02:45.106905Z","shell.execute_reply.started":"2022-01-30T10:02:44.576048Z","shell.execute_reply":"2022-01-30T10:02:45.106251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But we can clearly see that Passengers Of Pclass 1 were given a very high priority while rescue. Even thouth the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%\n우리는 구조하는 동안 피클래스 1의 승객들에게 우선순위가 주어졌음을 알고 있다. 피클래스 3의 승객 수가 훨씬 더 많았음에도 불구하고 생존자 수는 25%정도로 여전히 매우 적다\n\nFor Pclass 1% survived is around 63% while for Pclass2 is around 48%. So money and status matters. Such a materialistic world.\n피클래스의 경우 1%의 생존율이 약 63퍼인 반면 2의 생존율은 약 48퍼다. 물질만능주의 ㅜㅜ\nLets Dive in little bit more and check for other interesting observations. Let's check survival rate with Sex and Pclass Together.\n조금 더 자세히 살펴보고 다른 흥미로운 관측치가 있는지 확인해보자. 성별과 피클래스를 같이 확인해보자","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins=True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:45.107942Z","iopub.execute_input":"2022-01-30T10:02:45.108269Z","iopub.status.idle":"2022-01-30T10:02:45.15862Z","shell.execute_reply.started":"2022-01-30T10:02:45.108241Z","shell.execute_reply":"2022-01-30T10:02:45.157707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:45.160183Z","iopub.execute_input":"2022-01-30T10:02:45.160825Z","iopub.status.idle":"2022-01-30T10:02:45.734679Z","shell.execute_reply.started":"2022-01-30T10:02:45.160781Z","shell.execute_reply":"2022-01-30T10:02:45.733707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use FactorPlto in this case, because they make the seperation of categorical values easy.\n범주형 값을 쉽게 분리할 수 있기 때문에 factorplot사용한다.\n\nLooking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\n크로스탭과 팩터플롯을 보면 피클래스1 여성 94명 중 3명만 사망해 피클래스1 여성 생존율이 약 95-96퍼임을 쉽게 알 수 있다 \nIt it evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n피클래스와 관계없이 구조 시 여성이 최우선적으로 고려되었다는 것이 명백하다. 심지어 피클래스1의 남성들도 생존율이 매우 낮음\n\nLooks like Pclass is also an important feature. Lets analyse other features.\n피클래스도 아주 중요한 특징인 것 같다. 다른 특성도 분석해보자","metadata":{}},{"cell_type":"markdown","source":"### Age -> Continuous Feature 나이 -> 연속형 특성","metadata":{}},{"cell_type":"code","source":"print('Oldest Passenger was of : ', data['Age'].max(), 'Years')\nprint('Youngestn Passenger was of : ', data['Age'].min(), 'Years')\nprint('Average Age on the ship : ', data['Age'].mean(), 'Years')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:45.736302Z","iopub.execute_input":"2022-01-30T10:02:45.736626Z","iopub.status.idle":"2022-01-30T10:02:45.745315Z","shell.execute_reply.started":"2022-01-30T10:02:45.736582Z","shell.execute_reply":"2022-01-30T10:02:45.744594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=data, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\nsns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=data, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_title(range(0, 110, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:45.746553Z","iopub.execute_input":"2022-01-30T10:02:45.746988Z","iopub.status.idle":"2022-01-30T10:02:46.216077Z","shell.execute_reply.started":"2022-01-30T10:02:45.746923Z","shell.execute_reply":"2022-01-30T10:02:46.21522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations :\n관측치\n\n1. The number of children increases with Pclass and the survival rate for passengers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n피클래스에 따라 어린이의 수가 증가하며, 10세 미만 승객(어린이)의 생존율은 피클래스에 관계없이 양호한 것으로 보인다. \n\n2. Survival chances for Passengers aged 20-50 from Pclass1 is high and is even better for Women.\n피클래스1의 20-50세 승객은 생존 확률이 높으며, 여성의 생존 확률이 더 높다\n\n3. For males, the survival chances decreases with an increase in age.\n남성의 경우 나이가 많을수록 생존 가능성이 낮아진다\n\nAs we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset.\n앞에서 살펴본 것처럼 나이 특성에는 177개의 null 값이 있습니다. 이러한 NaN 값을 대체하기 위해 데이터셋의 평균을 할당할 수 있다.\n\nBut the problem is, there were many people with many different ages. We just cant assign a 4year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie?\n하지만 문제는 많은 다양한 연령대의 사람들이 있었다. 4살 아이를 평균 나이인 29세로 배정할 수가 없다. \n\nBingo !!!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective qroups.\n우리는 그래서 이름 특성을 확인할 수 있다. 특징을 보면, 이름들이 미스터 미세스등등 있으니까 이 사람들의 평균값을 각각 그룹에 할당할 수 있다\n\n\"What's in a Name??\" ------> Feature\n이름에 특성이 있다","metadata":{}},{"cell_type":"code","source":"data['Initial']=0\nfor i in data:\n    data['Initial'] = data.Name.str.extract('([A-Za-z]+)\\.') # 이름으로부터 이니셜을 추출","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.221475Z","iopub.execute_input":"2022-01-30T10:02:46.221723Z","iopub.status.idle":"2022-01-30T10:02:46.282753Z","shell.execute_reply.started":"2022-01-30T10:02:46.221686Z","shell.execute_reply":"2022-01-30T10:02:46.282062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay so here we are using the Regex:[A-Za-z]+).. So what it does is, it looks for strings which lie between A-Z or a-z and followed by a.(dot).So we successfully extract the Initials from the Name.\n이름으로부터 이니셜을 성공적으로 추출했음","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap='summer_r') # 이니셜과 성별 체크","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.283963Z","iopub.execute_input":"2022-01-30T10:02:46.284383Z","iopub.status.idle":"2022-01-30T10:02:46.332514Z","shell.execute_reply.started":"2022-01-30T10:02:46.284337Z","shell.execute_reply":"2022-01-30T10:02:46.331679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values.\nMlle나 Mme 처럼 틀린 이니셜이 있다. 이것들을 Miss같은 값으로 대체하자","metadata":{}},{"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.333845Z","iopub.execute_input":"2022-01-30T10:02:46.335084Z","iopub.status.idle":"2022-01-30T10:02:46.34264Z","shell.execute_reply.started":"2022-01-30T10:02:46.335046Z","shell.execute_reply":"2022-01-30T10:02:46.341962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby('Initial')['Age'].mean() # 이니셜로 평균 연령을 확인합시다.","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.34383Z","iopub.execute_input":"2022-01-30T10:02:46.344666Z","iopub.status.idle":"2022-01-30T10:02:46.356094Z","shell.execute_reply.started":"2022-01-30T10:02:46.344624Z","shell.execute_reply":"2022-01-30T10:02:46.355152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Filling NaN Ages NaN 나이를 채우자","metadata":{}},{"cell_type":"code","source":"# 평균 연령의 cell값을 사용하여 nan 값 할당\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.357362Z","iopub.execute_input":"2022-01-30T10:02:46.35769Z","iopub.status.idle":"2022-01-30T10:02:46.371415Z","shell.execute_reply.started":"2022-01-30T10:02:46.357662Z","shell.execute_reply":"2022-01-30T10:02:46.370535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any() # null 값은 최종적으로 남아 있지 않다","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.372867Z","iopub.execute_input":"2022-01-30T10:02:46.373214Z","iopub.status.idle":"2022-01-30T10:02:46.386062Z","shell.execute_reply.started":"2022-01-30T10:02:46.373172Z","shell.execute_reply":"2022-01-30T10:02:46.3854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived=0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1], color='green', bins=20, edgecolor='black')\nax[1].set_title('Survived = 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.38701Z","iopub.execute_input":"2022-01-30T10:02:46.38767Z","iopub.status.idle":"2022-01-30T10:02:46.871848Z","shell.execute_reply.started":"2022-01-30T10:02:46.387637Z","shell.execute_reply":"2022-01-30T10:02:46.870891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations : \n1. The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy)\n유아가 먼저 구조되었다\n\n2. The oldest Passenger was saved(80 years)\n최고령 승객 구조(80)\n\n3. Maximum number of deaths were in the age group of 30-40\n최대 사망자 수는 30-40세였다.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', col='Initial', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:46.873553Z","iopub.execute_input":"2022-01-30T10:02:46.873868Z","iopub.status.idle":"2022-01-30T10:02:48.082173Z","shell.execute_reply.started":"2022-01-30T10:02:46.873827Z","shell.execute_reply":"2022-01-30T10:02:48.081348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Women and Child first policy thus holds true irrespective of the class\n그러므로 여성과 아동 우선 정책은 계급에 관계없이 적용된다","metadata":{}},{"cell_type":"markdown","source":"### Embarked ---> Categorical Value\n항구 --> 범주형 ","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Embarked, data.Pclass], [data.Sex,data.Survived], margins=True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:48.083701Z","iopub.execute_input":"2022-01-30T10:02:48.084196Z","iopub.status.idle":"2022-01-30T10:02:48.15898Z","shell.execute_reply.started":"2022-01-30T10:02:48.084153Z","shell.execute_reply":"2022-01-30T10:02:48.158165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Chances for Survival by Port Embarkation\n항구에 의한 생존 가능성","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Embarked', 'Survived', data=data)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:48.160559Z","iopub.execute_input":"2022-01-30T10:02:48.160878Z","iopub.status.idle":"2022-01-30T10:02:48.513667Z","shell.execute_reply.started":"2022-01-30T10:02:48.160835Z","shell.execute_reply":"2022-01-30T10:02:48.512864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The chances for survival for Port C is highest around 0.55 while it is lowest for S\n포트 C의 생존 확률은 0.55정도로 가장 높고 S의 생존 확률은 가장 낮다","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:48.515215Z","iopub.execute_input":"2022-01-30T10:02:48.51611Z","iopub.status.idle":"2022-01-30T10:02:49.123716Z","shell.execute_reply.started":"2022-01-30T10:02:48.516062Z","shell.execute_reply":"2022-01-30T10:02:49.123014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations : \n\n1. Maximum passengers boared from S. Majority of them being from Pclass3 최대 승객은 S에서 탑승했다. 대부분이 Pclass3이다.\n\n2. The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers. \nC에서 온 승객 중 상당수는 살아남은 것으로 보아 운이 좋은 것 같다\n\n3. The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive\nS는 부자들이 많이 탄 항구같다. 이곳도 생존률이 낮다. 왜냐면 대략 81퍼의 피클래스3 승객들이 살지 못했기 때문이다.\n\n4. Port Q had almost 95% of the passengers were from Pclass3\nQ항구는 거의 95퍼의 승객들이 피클래스3이다.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:49.125363Z","iopub.execute_input":"2022-01-30T10:02:49.125935Z","iopub.status.idle":"2022-01-30T10:02:50.401729Z","shell.execute_reply.started":"2022-01-30T10:02:49.12589Z","shell.execute_reply":"2022-01-30T10:02:50.401041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observation : \n1. The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass\n피클래스1과 2의 생존확률은 클래스에 상관없이 거의 1이다\n\n2. Port S looks to be very unlucky for Pclass3 Passengers as the survival rate for both men and women is very low(Money Matters)\n피클래스3 승객에게 s항구는 남녀 모두 생존률이 매우 낮아 불운한 것으로 보인다.\n(돈 문제?)\n\n3. Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3 \nQ항구는 남자에게 불운해 보인다. 거의 모두가 피클래스3이다","metadata":{}},{"cell_type":"markdown","source":"#### Filling Embarked NaN\nAs we saw that maximum passenger boarded from Port S, we replace NaN with S ","metadata":{}},{"cell_type":"code","source":"data['Embarked'].fillna('S', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:50.403325Z","iopub.execute_input":"2022-01-30T10:02:50.403887Z","iopub.status.idle":"2022-01-30T10:02:50.410069Z","shell.execute_reply.started":"2022-01-30T10:02:50.403843Z","shell.execute_reply":"2022-01-30T10:02:50.408982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Embarked.isnull().any() # Finally no NaN values","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:50.411363Z","iopub.execute_input":"2022-01-30T10:02:50.411588Z","iopub.status.idle":"2022-01-30T10:02:50.426537Z","shell.execute_reply.started":"2022-01-30T10:02:50.411551Z","shell.execute_reply":"2022-01-30T10:02:50.426003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SibSp -> Discrete Feature 이산형 특성\n\nThis feature represents whether a person is alone or with his family members.\n이 특성은 승객이 혼자인지 아니면 가족들과 함께였는지를 나타낸다\n\nSibling = brother, sister, stepbrother, stepsister\n형제 = 형제, 자매, 의붓형제\n\nSpouse = husband, wife\n남편, 부인","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.SibSp], data.Survived).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:50.427641Z","iopub.execute_input":"2022-01-30T10:02:50.427894Z","iopub.status.idle":"2022-01-30T10:02:50.456703Z","shell.execute_reply.started":"2022-01-30T10:02:50.427866Z","shell.execute_reply":"2022-01-30T10:02:50.455999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp', 'Survived', data=data, ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.pointplot('SibSp', 'Survived', data=data, ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:50.458022Z","iopub.execute_input":"2022-01-30T10:02:50.458701Z","iopub.status.idle":"2022-01-30T10:02:51.301637Z","shell.execute_reply.started":"2022-01-30T10:02:50.458655Z","shell.execute_reply":"2022-01-30T10:02:51.300725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:51.303165Z","iopub.execute_input":"2022-01-30T10:02:51.303497Z","iopub.status.idle":"2022-01-30T10:02:51.335694Z","shell.execute_reply.started":"2022-01-30T10:02:51.303453Z","shell.execute_reply":"2022-01-30T10:02:51.334993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n\nThe barplot and factorplot shows that if a passenger is alone on board with no siblings, he have 34.5% survival rate. The graph roughly decreases of the number of siblings increase. This makes sense. That is, if i have a family on board, i will to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is 0%. The reason may be Pclass??\n막대그림 및 팩터플롯을 보면 승객이 형제 없이 혼자 탑승한 경우 생존율이 34.5다. 그래프는 형제 수가 증가할수록 감소한다. 말이 되는게 만약 가족이 있다면, 나를 구하는 대신 그들을 먼저 구할것이다. 놀랍게도 5-8인 가족의 생존율은 0%다. 그 이유는 피클래스 때문일 수 있음\n\nThe reason is Pclass. The crosstab shows that with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass(>3) died.\n크로스탭이 sibsp>3이 모두 피클래스 3에 있음을 보여준다. 피클래스의 대가족이 모두 사망할것이라고 예상한다.","metadata":{}},{"cell_type":"markdown","source":"### Parch 부모+자식","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:51.336913Z","iopub.execute_input":"2022-01-30T10:02:51.337171Z","iopub.status.idle":"2022-01-30T10:02:51.365143Z","shell.execute_reply.started":"2022-01-30T10:02:51.337141Z","shell.execute_reply":"2022-01-30T10:02:51.364238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The crosstab again shows that larger families were in Pclass3. 크로스탭은 피클래스3에 대가족이 많다는 것을 보여준다","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.pointplot('Parch', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:51.366809Z","iopub.execute_input":"2022-01-30T10:02:51.367139Z","iopub.status.idle":"2022-01-30T10:02:52.132198Z","shell.execute_reply.started":"2022-01-30T10:02:51.367092Z","shell.execute_reply":"2022-01-30T10:02:52.131331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n\nHere too there results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n결과가 꽤 비슷하다. 부모님과 탄 승객들은 생존가능성이 높다. 하지만 숫자가 올라갈수록 줄어든다\n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and chances for survival decreases when somebody has >4 parents on the ship.\n1-3 부모님과 배를 탄 사람들은 생존 가능성이 좋다. 혼자 있는것 또한 치명적이고 누군가 4 이상의 부모가 있을 때 생존 가능성이 줄어든다.","metadata":{}},{"cell_type":"markdown","source":"### Fare -> Continuous Feature 요금 -> 연속적 특성","metadata":{}},{"cell_type":"code","source":"print('Highest Fare was : ', data['Fare'].max())\nprint('Lowest Fare was : ', data['Fare'].min())\nprint('Average Fare was : ', data['Fare'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:52.133309Z","iopub.execute_input":"2022-01-30T10:02:52.133845Z","iopub.status.idle":"2022-01-30T10:02:52.141422Z","shell.execute_reply.started":"2022-01-30T10:02:52.133807Z","shell.execute_reply":"2022-01-30T10:02:52.140323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lowest fare is 0.0. Wow a free luxorious ride.\n가장 낮은 요금은 0.0. 무료로 탔다","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:52.142783Z","iopub.execute_input":"2022-01-30T10:02:52.143027Z","iopub.status.idle":"2022-01-30T10:02:53.02919Z","shell.execute_reply.started":"2022-01-30T10:02:52.142995Z","shell.execute_reply":"2022-01-30T10:02:53.028265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning.\n피클래스1의 승객 요금에 큰 분포가 있어 보이며 표준이 감소하는 것으로 봐서 이러한 분포는 계속 감소하고 있음. 이것은 연속적이기 때문에, 우리는 바이닝을 통해서 값을 이산값으로 변환할 수 있다","metadata":{}},{"cell_type":"markdown","source":"#### Observations in a Nutshell for all features:\n모든 특성에 대한 요약\n\nsex : the chance of survival for women is high as compared to men.\n성별 : 여성의 생존가능성은 남성에 비해 높다.\n\nPclass : there is a visible trend that being a 1st class passenger gives you better chances of survival. The survival rate for Pclass3 is very low. For women, the chance ofsurvival from Pclass1 is almost 1 and high to for those from Pclass 2. Money wins!\n피클래스 : 퍼스트 클래스의 승객들은 더 높은 생존 가능성이 주어진다는 추세가 있다. 피클래스3의 생존률은 저조하다. 여성의 경우, 피클래스 1의 생존 가능성은 거의 1이고 피클래스 2는 매우 높다. 돈이 최고\n\nAge : Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n나이 : 아이들은 5-10세 미만일 경우 생존 가능성이 높다. 15-35 사이의 승객들은 많이 죽었다.\n\nEmbarked : This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passenger got up at S. Passengers at Q were all from Pclass3.\n항구 : 매우 흥미로운 피쳐, C에서의 생존 가능성은 피클래스 1 승객의 대다수가 탄 S보다는 높아보인다. Q의 승객은 거의 피클래스3이었다.\n\nParch + SibSp : Having 1-2 siblings, spouse on board or 1-3 Parents shows a greater chance of probabilty rather than being alone or having a large family travelling with you.\n부모 형제 : 1-2명의 형제자매, 배우자 또는 1-3명의 부모를 동반하는 것이 혼자나 대가족이 함께 여행하는 것보다 더 높은 가능성을 보여준다","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:06:08.881292Z","iopub.execute_input":"2022-01-30T08:06:08.881798Z","iopub.status.idle":"2022-01-30T08:06:08.886084Z","shell.execute_reply.started":"2022-01-30T08:06:08.881761Z","shell.execute_reply":"2022-01-30T08:06:08.88519Z"}}},{"cell_type":"markdown","source":"### Correlation Between The Features 변수들 간 상관관계","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2) #data.corr() ---> correlation matrix\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:53.030724Z","iopub.execute_input":"2022-01-30T10:02:53.030943Z","iopub.status.idle":"2022-01-30T10:02:53.464328Z","shell.execute_reply.started":"2022-01-30T10:02:53.030917Z","shell.execute_reply":"2022-01-30T10:02:53.463305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpreting The Heatmap\n히트맵 해석\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n첫번째로 주목할 점은 알파벳이나 문자열의 상관관계가 분명하지 않기 때문에 숫자적인 특징만 비교한다는 것. 그림을 이해하기 전에 정확한 상관관계가 무엇인지 알아보자\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n양의 상관관계 : A의 증가가 피쳐 B의 증가로 이어지면 둘은 양의 상관관계, 1은 완벽한 양의 상관관계를 의미한다\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n음의 상관관계 : A B가 서로 감소로 이어지면 음의 상관관계, -1은 완벽한 음의 상관관계\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n이제 두 기능이 완벽하게 상관되어 있으므로 하나가 증가하면 다른 하나도 증가한다고 가정해보자. 두 특징 모두 매우 유사한 정보를 포함하고 있으며 정보의 편차가 거의 없다. 두 가지 모두 거의 동일한 정보를 가지고 있기 때문에 이를 multicolinarity라고 한다\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n둘이 중복되는데 둘 다 사용해야 할까? 모델을 만들거나 훈련시키기 위해 우리는 중복 기능을 제거해야 한다. 훈련 시간을 줄이기 위해\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features.\n위의 그래프를 통해 우리는 특징들이 크게 상관성이 없다는 것을 알 수 있다. 가장 높은 상관관계는 sibsp와 parch. 0.41이다. 그래서 우리는 모든 특성을 사용할 수 있다","metadata":{}},{"cell_type":"markdown","source":"## Part2: Feature Engineering and Data Cleaning\n피쳐 엔지니어링 및 데이터 정리\n\nNow what is feature engineering ?\n피쳐 엔지니어링이 무엇일까?\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n여러 피쳐가 있는 데이터셋이 주어졌을 때, 언제나 모든 특성이 중요한것은 아니다. 제거해야할 중복 특성이 있을 수 있다. 또한 우리는 다른 특징에서 얻은 정보를 통해 새로운 특징을 얻거나 추가할 수 있다.\n\nAn example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling. \n예를 들어 이름 피쳐에서 이니셜 피쳐를 만들수 있다. 몇가지 새로운 특성을 추가하고 몇가지 기능을 제거할 수 있을지 살펴보자. 기존 피쳐 또한 모델링에 적합한 형태로 변환하자","metadata":{}},{"cell_type":"markdown","source":"### Age_band\n\nProblem With Age Feature:\nAs I have mentioned earlier that Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.\n나이 특성의 문제 : 앞서 말했듯이 나이는 연속형 피쳐다. 머신러닝 모델에서 연속형 변수는 문제가 있음\n\nEg:If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.\n예 : 스포츠인을 성별에 따라 분류하거나 배열하라고 하면 쉽게 남녀로 구분할 수 있다.\n\nNow if I say to group them by their Age, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.\n만약 그들을 시대별로 분류해야 한다면, 어떻게 할것인가? 그들이 30대라면, 연령 값은 30명이 될 수 있다. 이게 문제\n\nWe need to convert these continous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n우리는 이런 연속형을 범주형으로 변환해야한다 비닝 또는 정규화를 통해 다양한 연령을 단일 빈으로 그룹하거나 값을 할당한다\n\nOkay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16. So bins of size 16.\n최대 연령은 80세니까 0-80의 범위를 5개로 나누자. 16 사이즈의 빈","metadata":{}},{"cell_type":"code","source":"data['Age_band'] = 0\ndata.loc[data['Age']<=16, 'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32), 'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48), 'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64), 'Age_band']=3\ndata.loc[data['Age']>64, 'Age_band']=4\ndata.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:53.465592Z","iopub.execute_input":"2022-01-30T10:02:53.465834Z","iopub.status.idle":"2022-01-30T10:02:53.491078Z","shell.execute_reply.started":"2022-01-30T10:02:53.465808Z","shell.execute_reply":"2022-01-30T10:02:53.490109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')\n# Checking the number of passengers in each band","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:53.492215Z","iopub.execute_input":"2022-01-30T10:02:53.492453Z","iopub.status.idle":"2022-01-30T10:02:53.507228Z","shell.execute_reply.started":"2022-01-30T10:02:53.492406Z","shell.execute_reply":"2022-01-30T10:02:53.506525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Age_band', 'Survived', data=data, col='Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:53.508322Z","iopub.execute_input":"2022-01-30T10:02:53.508526Z","iopub.status.idle":"2022-01-30T10:02:54.46818Z","shell.execute_reply.started":"2022-01-30T10:02:53.5085Z","shell.execute_reply":"2022-01-30T10:02:54.467462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"true that.. the survival rate decrease as the age increases irrespective of the Pclass.\n생존율은 피클래스에 관계없이 나이가 증가함에 따라 감소한다","metadata":{}},{"cell_type":"markdown","source":"### Family_Size and Alone\n\nAt this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not\n이 시점에서, 패밀리 사이즈와 얼론 이라는 새로운 피쳐를 만들어보자. 이 특성은 parch 와 sibsp의 합이다. 생존율이 가족 규모와 관련이 있는지 확인할 수 있도록 결합된 데이터 제공. 얼론은 혼자인지 아닌지 알수 있다. ","metadata":{}},{"cell_type":"code","source":"data['Family_Size'] = 0\ndata['Family_Size'] = data['Parch']+data['SibSp'] #family size\ndata['Alone'] = 0\ndata.loc[data.Family_Size==0, 'Alone']=1 #Alone\n\nf, ax = plt.subplots(1,2, figsize=(18, 6))\nsns.pointplot('Family_Size', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Family_size vs Survived')\nsns.pointplot('Alone', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:54.469163Z","iopub.execute_input":"2022-01-30T10:02:54.469425Z","iopub.status.idle":"2022-01-30T10:02:55.160242Z","shell.execute_reply.started":"2022-01-30T10:02:54.469389Z","shell.execute_reply":"2022-01-30T10:02:55.159383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Family_Size=0 means that the passeneger is alone. Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further.\n패밀리 사이즈=0은 승객이 혼자라는 것을 의미. 그럴 경우 생존 가능성이 매우 낮다. 가족수가 4보다 크면 생존 기회가 줄어든다. 이것은 모델에 중요한 특성이 될 것으로 보인다. 더 검토해보자","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Alone', 'Survived', data=data, hue='Sex', col='Pclass')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:55.161581Z","iopub.execute_input":"2022-01-30T10:02:55.161889Z","iopub.status.idle":"2022-01-30T10:02:56.313987Z","shell.execute_reply.started":"2022-01-30T10:02:55.161847Z","shell.execute_reply":"2022-01-30T10:02:56.313136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\n가족이 있는 여성보다 혼자 있을 확률이 높은 피클래스3을 제외하면 성별과 피클래스를 불문하고 혼자있는것이 유해하다는 것이 눈에 띔","metadata":{}},{"cell_type":"markdown","source":"### Fare_Range 요금범위\n\nSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use pandas.qcut.\n요금 또한 연속적 특성이기 때문에, 우리는 순서형으로 변환해야 한다. 이걸위해 판다스의 큐컷을 사용할 것이다\n\nSo what qcut does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.\n큐컷은 우리가 통과한 빈의 수에 따라 값을 분할하거나 배열한다. 따라서 5개의 빈을 통과하면 값이 5개의 개별 빈 또는 값 범위로 균등하게 배열된다","metadata":{}},{"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:56.319679Z","iopub.execute_input":"2022-01-30T10:02:56.31993Z","iopub.status.idle":"2022-01-30T10:02:56.341651Z","shell.execute_reply.started":"2022-01-30T10:02:56.319902Z","shell.execute_reply":"2022-01-30T10:02:56.34082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As discussed above, we can clearly see that as the fare_range increases, the chances of survival increases.\n위에서 말했듯이, 우리는 요금범위가 높아질수록 생존 가능성이 높다는 것을 알 수 있다.\n\nNow we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in Age_Band\n현재로서는 요금 범위값을 전달할수 없다. 우리는 그것을 에이지밴드 처럼 싱글톤 값으로 변환해야한다","metadata":{}},{"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:56.342657Z","iopub.execute_input":"2022-01-30T10:02:56.343325Z","iopub.status.idle":"2022-01-30T10:02:56.353764Z","shell.execute_reply.started":"2022-01-30T10:02:56.34329Z","shell.execute_reply":"2022-01-30T10:02:56.352909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:56.355056Z","iopub.execute_input":"2022-01-30T10:02:56.355612Z","iopub.status.idle":"2022-01-30T10:02:56.947018Z","shell.execute_reply.started":"2022-01-30T10:02:56.355561Z","shell.execute_reply":"2022-01-30T10:02:56.946025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, as the Fare_cat increases, the survival chances increases. This feature may become an important feature during modeling along with the Sex.\n분명히, farecat이 증가할수록 생존율이 높아진다. 이 특성은 성별과 함께 중요한 특징이 된다","metadata":{}},{"cell_type":"markdown","source":"### Converting String Values into Numeric\nsince we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.\n문자열 값을 숫자로 변환\n\n머신러닝 모델에 문자열을 전달할 수 없기 때문에, 성별, 탑승처럼 숫자값으로 변환해야한다","metadata":{}},{"cell_type":"code","source":"data['Sex'].replace(['male', 'female'], [0,1], inplace=True)\ndata['Embarked'].replace(['S','C','Q'], [0,1,2], inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'], [0,1,2,3,4], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:56.948756Z","iopub.execute_input":"2022-01-30T10:02:56.949123Z","iopub.status.idle":"2022-01-30T10:02:56.961513Z","shell.execute_reply.started":"2022-01-30T10:02:56.949081Z","shell.execute_reply":"2022-01-30T10:02:56.960601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping UnNeeded Features 불필요한 특성 제거\n\nName--> We don't need name feature as it cannot be converted into any categorical value.\n이름 = 우리는 어떤 범주형 값으로 변환할 수가 없기 때문에 필요없다\n\nAge--> We have the Age_band feature, so no need of this.\n나이 = 우리는 에이지밴드가 있으니까. 필요없다\n\nTicket--> It is any random string that cannot be categorised.\n표 = 이것은 랜덤한 문자기 때문에 범주화할수가 없음\n\nFare--> We have the Fare_cat feature, so unneeded\n요금 = 우리는 farecat특성이 있으니까 필요없다\n\nCabin--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n카빈 = nan값이 넘 많다. 필요없는 특성\n\nFare_Range--> We have the fare_cat feature.\n페어 레인지 = farecat이 있어서 필요없음\n\nPassengerId--> Cannot be categorised.\n승객 id = 범주화할수 없음","metadata":{}},{"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:56.963272Z","iopub.execute_input":"2022-01-30T10:02:56.963591Z","iopub.status.idle":"2022-01-30T10:02:57.744859Z","shell.execute_reply.started":"2022-01-30T10:02:56.963548Z","shell.execute_reply":"2022-01-30T10:02:57.744015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the above correlation plot, we can see some positively related features. Some of them being SibSp andd Family_Size and Parch and Family_Size and some negative ones like Alone and Family_Size.\n위의 상관관계도를 보면, 양의 관계를 가진 몇가지 특성을 알 수 있다. 일부는 sibsp family size이고 일부는 parch familysize다. alone과 familysize와 같은 부정적인 것이다.\n","metadata":{}},{"cell_type":"markdown","source":"## Part3: Predictive Modeling 모델링\n\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\n우리는 eda를 통해 몇가지 인사이트를 얻었다. 하지만 그것으로는 승객이 생존할지 여부를 정확하게 예측할 수 없다. 이제 우리는 알고리즘을 통해 승객의 생존여부를 예측해보겠다. 모델을 만들기 위한 알고리즘은 다음과 같다\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression","metadata":{}},{"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.74644Z","iopub.execute_input":"2022-01-30T10:02:57.746982Z","iopub.status.idle":"2022-01-30T10:02:57.753886Z","shell.execute_reply.started":"2022-01-30T10:02:57.746918Z","shell.execute_reply":"2022-01-30T10:02:57.753136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.755237Z","iopub.execute_input":"2022-01-30T10:02:57.755771Z","iopub.status.idle":"2022-01-30T10:02:57.771837Z","shell.execute_reply.started":"2022-01-30T10:02:57.755738Z","shell.execute_reply":"2022-01-30T10:02:57.771178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_X.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.773671Z","iopub.execute_input":"2022-01-30T10:02:57.77403Z","iopub.status.idle":"2022-01-30T10:02:57.783976Z","shell.execute_reply.started":"2022-01-30T10:02:57.773988Z","shell.execute_reply":"2022-01-30T10:02:57.783079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Radial Support Vector Machines(rbf_SVM)","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.785173Z","iopub.execute_input":"2022-01-30T10:02:57.78541Z","iopub.status.idle":"2022-01-30T10:02:57.815716Z","shell.execute_reply.started":"2022-01-30T10:02:57.785383Z","shell.execute_reply":"2022-01-30T10:02:57.814782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Support Vector Machine(linear-SVM)\n","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.816867Z","iopub.execute_input":"2022-01-30T10:02:57.817141Z","iopub.status.idle":"2022-01-30T10:02:57.835523Z","shell.execute_reply.started":"2022-01-30T10:02:57.81711Z","shell.execute_reply":"2022-01-30T10:02:57.83443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression\n","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.836643Z","iopub.execute_input":"2022-01-30T10:02:57.83686Z","iopub.status.idle":"2022-01-30T10:02:57.859485Z","shell.execute_reply.started":"2022-01-30T10:02:57.836833Z","shell.execute_reply":"2022-01-30T10:02:57.858816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree","metadata":{}},{"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.860402Z","iopub.execute_input":"2022-01-30T10:02:57.861107Z","iopub.status.idle":"2022-01-30T10:02:57.872011Z","shell.execute_reply.started":"2022-01-30T10:02:57.861076Z","shell.execute_reply":"2022-01-30T10:02:57.871232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### K-Nearest Neighbours(KNN)","metadata":{}},{"cell_type":"code","source":"model=KNeighborsClassifier() \nmodel.fit(train_X,train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.873566Z","iopub.execute_input":"2022-01-30T10:02:57.874165Z","iopub.status.idle":"2022-01-30T10:02:57.904015Z","shell.execute_reply.started":"2022-01-30T10:02:57.874121Z","shell.execute_reply":"2022-01-30T10:02:57.903124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. The default value is 5. Lets check the accuracies over various values of n_neighbours.\nknn이웃 속성값을 변경하면 knn모델의 정확도가 변경된다. 기본값은 5다. knn의 다양한 값에 대한 정확도를 확인하겠다.","metadata":{}},{"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:57.905286Z","iopub.execute_input":"2022-01-30T10:02:57.906149Z","iopub.status.idle":"2022-01-30T10:02:58.243768Z","shell.execute_reply.started":"2022-01-30T10:02:57.906105Z","shell.execute_reply":"2022-01-30T10:02:58.242964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gaussian Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:58.245347Z","iopub.execute_input":"2022-01-30T10:02:58.246084Z","iopub.status.idle":"2022-01-30T10:02:58.259536Z","shell.execute_reply.started":"2022-01-30T10:02:58.246039Z","shell.execute_reply":"2022-01-30T10:02:58.258914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forests","metadata":{}},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:58.260843Z","iopub.execute_input":"2022-01-30T10:02:58.2617Z","iopub.status.idle":"2022-01-30T10:02:58.488484Z","shell.execute_reply.started":"2022-01-30T10:02:58.261658Z","shell.execute_reply":"2022-01-30T10:02:58.487642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"he accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n모형의 정확성만이 분류기의 견고함을 결정하는 유일한 요인은 아니다. 분류기가 훈련 데이터에 의해 훈련되고 테스트 데이터에 대해 테스트되며 90퍼의 정확도를 기록한다고 생각해보자\n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as model variance.\n매우 정확한 분류기처럼 보이지만, 새로 나오는 모든 테스트세트에 대해 90퍼가 될 수 있는지 확인할 수 있을까? 아니다. 그 이유는 분류기가 자신을 훈련시키기 위해 어떤 인스턴스를 사용할지 결정할 수 없기 때문이다. 훈련과 테스트데이터가 바뀌면 정확도도 달라질것.. 증가하거나 감소할 수 있는데 이를 모델 분산? 이라고 한다.\n\nTo overcome this and get a generalized model,we use Cross Validation.\n이것의 극복과 일반화된 모델을 얻기 위해 교차검증을 사용한다","metadata":{}},{"cell_type":"markdown","source":"#### Cross Validation\n\nMany a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n데이터가 불균형한 경우가 많다. 즉 클래스1의 인스턴스 수는 많지만 다른 클래스의 인스턴스 수는 적을 수 있음. 따라서 데이터 셋의 각 인스턴스에서 알고리즘을 교육하고 테스트해야 한다. 그런 다음 데이터 세트의 모든 정확도의 평균을 구할 수 있다.\n\n1) The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n케이 폴드 교차검증은 먼저 데이터 집합을 k하위 집합으로 나누는 방식으로 작동한다\n\n2) Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n데이터 세트를 5개로 나눈다고 가정하면. 우리는 테스트를 위해 1개의 파트를 예약하고 4개 파트에 걸쳐 알고리즘을 훈련한다\n\n3) We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n각 반복에서 테스트 부분을 변경하고 다른 부분에 대해 알고리즘을 훈련하는 과정을 진행한다. 그런 다음 알고리즘의 평균 정확도를 얻기 위해 정확도와 오류를 평균화한다\n\nThis is called K-Fold Cross Validation.\n이걸 케이폴드 교차검증이라고 함\n\n4) An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.\n알고리즘은 일부 훈련 데이터에 대해 데이터 세트를 과소 적합시킬 수 있으며 때로는 과대적합시킬 수 있다. 따라서 교차 검증을 통해 우리는 일반화된 모델을 만들 수 있다","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:02:58.49006Z","iopub.execute_input":"2022-01-30T10:02:58.490546Z","iopub.status.idle":"2022-01-30T10:03:01.501352Z","shell.execute_reply.started":"2022-01-30T10:02:58.490501Z","shell.execute_reply":"2022-01-30T10:03:01.500339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:03:01.503197Z","iopub.execute_input":"2022-01-30T10:03:01.503525Z","iopub.status.idle":"2022-01-30T10:03:01.748811Z","shell.execute_reply.started":"2022-01-30T10:03:01.503484Z","shell.execute_reply":"2022-01-30T10:03:01.748138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:03:01.749855Z","iopub.execute_input":"2022-01-30T10:03:01.750487Z","iopub.status.idle":"2022-01-30T10:03:01.969412Z","shell.execute_reply.started":"2022-01-30T10:03:01.750454Z","shell.execute_reply":"2022-01-30T10:03:01.968568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.\n정확도는 때때로 불균형으로 인해 잘못될 수 있다. 우리는 컨퓨전 매트릭스를 통해 요약된 결과를 얻을 수 있다. 모델이 어디서 잘못됐는지, 어떤 클래스가 잘못 예측했는지 보여준다","metadata":{}},{"cell_type":"markdown","source":"#### Confusion Matrix\n\nIt gives the number of correct and incorrect classifications made by the classifier.\n분류기에 의해 생성된 정확한 분류와 부정확한 분류의 수를 제공한다","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:03:01.970665Z","iopub.execute_input":"2022-01-30T10:03:01.970929Z","iopub.status.idle":"2022-01-30T10:03:07.380695Z","shell.execute_reply.started":"2022-01-30T10:03:01.970902Z","shell.execute_reply":"2022-01-30T10:03:07.379798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpreting Confusion Matrix\n해석\n\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for rbf-SVM:\n왼쪽 대각선은 각 클래스에 의해 수행된 올바른 예측의 수, 오른쪽 대각선은 잘못된 예측의 수를 나타낸다. rbfsvm에 대한 첫번째 플롯을 보자\n\n1) The no. of correct predictions are 491(for dead) + 247(for survived) with the mean CV accuracy being (491+247)/891 = 82.8% which we did get earlier.\n정확한 예측의 수는 491(사망한 경우) + 247(생존한 경우)이며, 평균 정확도는 491+247 나누기 891 = 82.8\n\n2) Errors--> Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n오류 = 58의 사망자를 생존자로 잘못 분류, 95명은 사망자로 분류함. 그러므로 살아남은 것으로 죽은 것을 예측했기 때문에 더 많은 실수를 했다\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.\n모든 행렬을 보면 죽은 승객을 정확하게 예측할 확률이 높지만 나이브베이즈는 살아남은 승객을 정확하게 예측할 확률이 높다고 할수있다","metadata":{}},{"cell_type":"markdown","source":"#### Hyper-Parameters Tuning \n\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.\n머신러닝 모델은 블랙박스와 같다. 이 블랙박스에는 몇가지 기본 매개변수 값이 있으며 더 나은 모델을 위해 조정하거나 변경할 수 있다. svm모델의 c감마처럼 그리고 서로 다른 분류기에 대해 비슷하게 다른 매개변수를 하이퍼 파라미터라고 하며 이를 조정해서 알고리즘의 학습속도를 변경하고 더 나은 모델을 얻을 수 있다. 이걸 하이퍼파라미터튜닝이라 한다\n\nWe will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests.\nsvm과 랜덤포레스트 등 젤 괜찮은 2개의 분류기에 대해 하이퍼 매개변수를 조정한다","metadata":{}},{"cell_type":"markdown","source":"#### SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:03:07.382125Z","iopub.execute_input":"2022-01-30T10:03:07.382742Z","iopub.status.idle":"2022-01-30T10:03:28.358459Z","shell.execute_reply.started":"2022-01-30T10:03:07.382693Z","shell.execute_reply":"2022-01-30T10:03:28.357575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forests","metadata":{}},{"cell_type":"code","source":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:03:28.359654Z","iopub.execute_input":"2022-01-30T10:03:28.359873Z","iopub.status.idle":"2022-01-30T10:04:16.681101Z","shell.execute_reply.started":"2022-01-30T10:03:28.359848Z","shell.execute_reply":"2022-01-30T10:04:16.680092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best score for Rbf-Svm is 82.82% with C=0.05 and gamma=0.1. For RandomForest, score is abt 81.8% with n_estimators=900.\nrbf-svm의 최고 점수로는 82.82로 C=0.05, 감마 = 0.1이다. 랜덤포레스트의 경우 점수는 81.8로 n_estimator=900이다","metadata":{}},{"cell_type":"markdown","source":"### Ensembling 앙상블\n\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\n앙상블은 모델의 정확성 또는 성능을 높이는 좋은 방법. 간단히 말해 그것은 다양한 간단한 모델들이 결합해 하나의 강력한 모델을 만드는 것\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is Ensembling, which improves the stability of the model. Ensembling can be done in ways like:\n우리가 핸드폰을 구매하고 싶고 다양한 매개 변수들을 바탕으로 많은 사람들에게 문의해보자. 그러면 서로 다른 모든 변수를 분석한 후 단일 제품에 대해 강력한 판단을 내릴 수 있다. 이것은 모델의 안정성을 높여주는 앙상블링.. 다음과 같은 방법이 있다\n\n1) Voting Classifier\n\n2) Bagging\n\n3) Boosting","metadata":{}},{"cell_type":"markdown","source":"#### Voting Classifier\nVoting Classifier\nIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types.\n이것은 많은 머신러닝 모델의 예측을 결합하는 가장 간단한 방법. 모든 서브모델의 예측에 기초한 평균 예측 결과를 제공한다. 서브모델이나 베이즈 모델은 모두 다양한 종류다","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(train_X,train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:04:16.682346Z","iopub.execute_input":"2022-01-30T10:04:16.682822Z","iopub.status.idle":"2022-01-30T10:04:30.616303Z","shell.execute_reply.started":"2022-01-30T10:04:16.682763Z","shell.execute_reply":"2022-01-30T10:04:30.61559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bagging\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\n배깅은 일반적인 앙상블 방식이다. 데이터 세트의 작은 파티션에 유사한 분류기를 적용한 다음 모든 예측의 평균을 구하는 방식으로 작동한다. 평균화로 인해 분산이 감소한다. 투표 분류기와 달리 배깅은 유사한 분류기를 사용\n\n#### Bagged KNN\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours.\n배깅은 분산이 높은 모형에서 가장 잘 작동한다. 이러한 예로 의사결정트리 또는 랜덤포레스트가 있다. 작은 k값으로 사용가능","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:04:30.61761Z","iopub.execute_input":"2022-01-30T10:04:30.617876Z","iopub.status.idle":"2022-01-30T10:04:58.640822Z","shell.execute_reply.started":"2022-01-30T10:04:30.617846Z","shell.execute_reply":"2022-01-30T10:04:58.64Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bagged DecisionTree","metadata":{}},{"cell_type":"code","source":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:04:58.643829Z","iopub.execute_input":"2022-01-30T10:04:58.644514Z","iopub.status.idle":"2022-01-30T10:05:01.821991Z","shell.execute_reply.started":"2022-01-30T10:04:58.644481Z","shell.execute_reply":"2022-01-30T10:05:01.820999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boosting\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\n부스팅은 분류기의 순차적 학습을 사용하는 앙상블 기법. 약한 모델의 단계적 향상이다. \n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\n모델은 먼저 전체 데이터세트에 대해 교육된다. 이제 모델은 어떤 예는 맞고 다른건 틀릴것이다. 이제 다음 반복에서 잘못 예측된 인스턴스에 더 집중하거나 더 많은 비중을 둔다. 따라서 잘못된 인스턴스를 정확하게 예측하려고 한다. 이 과정은 연속적이고 정확도의 한계에 도달할 때까지 새로운 분류기가 모델에 추가된다.","metadata":{}},{"cell_type":"markdown","source":"#### AdaBoost(Adaptive Boosting)\nThe weak learner or estimator in this case is a Decsion Tree. But we can change the dafault base_estimator to any algorithm of our choice.\n이 약한 학습기 또는 예측기는 디시젼 트리. 하지만 우리는 디폴트값을 우리가 선택한 어떤 알고리즘으로도 변경할 수  있다 ...?? 먼솔","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:05:01.823088Z","iopub.execute_input":"2022-01-30T10:05:01.823293Z","iopub.status.idle":"2022-01-30T10:05:06.03194Z","shell.execute_reply.started":"2022-01-30T10:05:01.823267Z","shell.execute_reply":"2022-01-30T10:05:06.031073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stochastic Gradient Boosting\nHere too the weak learner is a Decision Tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:05:06.033059Z","iopub.execute_input":"2022-01-30T10:05:06.033281Z","iopub.status.idle":"2022-01-30T10:05:11.331802Z","shell.execute_reply.started":"2022-01-30T10:05:06.033254Z","shell.execute_reply":"2022-01-30T10:05:11.330914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xg\nxg.set_config(verbosity=0) # 오류\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:05:11.332836Z","iopub.execute_input":"2022-01-30T10:05:11.333101Z","iopub.status.idle":"2022-01-30T10:05:36.818835Z","shell.execute_reply.started":"2022-01-30T10:05:11.333063Z","shell.execute_reply":"2022-01-30T10:05:36.818158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got the highest accuracy for AdaBoost. We will try to increase it with Hyper-Parameter Tuning\n우리는 adaboost의 정확도를 제일 높였다. 하이퍼 파라미터 튜닝으로 늘려보자\n","metadata":{}},{"cell_type":"markdown","source":"#### Hyper-Parameter Tuning for AdaBoost","metadata":{}},{"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:05:36.822541Z","iopub.execute_input":"2022-01-30T10:05:36.82485Z","iopub.status.idle":"2022-01-30T10:16:54.984665Z","shell.execute_reply.started":"2022-01-30T10:05:36.824805Z","shell.execute_reply":"2022-01-30T10:16:54.983763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum accuracy we can get with AdaBoost is 83.16% with n_estimators=200 and learning_rate=0.05","metadata":{}},{"cell_type":"markdown","source":"#### Confusion Matrix for the Best Model","metadata":{}},{"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:16:54.985929Z","iopub.execute_input":"2022-01-30T10:16:54.986268Z","iopub.status.idle":"2022-01-30T10:16:59.414631Z","shell.execute_reply.started":"2022-01-30T10:16:54.986233Z","shell.execute_reply":"2022-01-30T10:16:59.413467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T10:16:59.415724Z","iopub.execute_input":"2022-01-30T10:16:59.41595Z","iopub.status.idle":"2022-01-30T10:17:04.556635Z","shell.execute_reply.started":"2022-01-30T10:16:59.415923Z","shell.execute_reply":"2022-01-30T10:17:04.555875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the important features for various classifiers like RandomForests, AdaBoost,etc.\n랜덤포레스트, adaboost등 다양한 분류기의 중요한 특징을 볼 수 있다\n\n\n#### Observations:\n1) Some of the common important features are Initial,Fare_cat,Pclass,Family_Size.\n일반적으로 중요한 특성으로는 이니셜, 요금카테고리, 피클래스, 가족수등이 있ㄷ\n\n2) The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. Sex looks to be important only in RandomForests.\n성별은 별다른 중요성을 부여하지 않는것처럼 보이는데, 성별과 피클래스가 결합된 것이 매우 좋은 차별화 요소를 주고 있다는 것은 충격적이다. 성별은 랜덤 포레스트에서만 중요해 보인다\n\nHowever, we can see the feature Initial, which is at the top in many classifiers.We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\n그러나 많은 분류기에서 맨위에 있는 특성으로 이니셜을 볼 수 있다. 우리는 이미 성별과 이니셜의 양의 상관관계를 봤기 때문에 둘 다 성별을 언급한다. \n\n3) Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp.\n마찬가지로 피클래스와 요금 카테고리는 승객 및 가족수와 alone parch sibsp의 상태를 나타낸다\n\nI hope all of you did gain some insights to Machine Learning. Some other great notebooks for Machine Learning are: \n\n끝 ....","metadata":{}}]}