{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n해당 커널은 한글 번역본입니당\n\n이 노트북은 PortoSerguro Competition에서 멋진 인사이트를 얻기 위해 작성 되었습니다.\n뿐만 아니라 데이터 모델링을 준비하기 위한 Tip과 Tricks들을 준비했습니다.\n\n목차는 다음과 같습니다.\n\n1. Visual inspection of your data\n2. Defining the metadata\n3. Descriptive statistics\n4. Handling imbalanced classes\n5. Data quality checks\n6. Exploratory data visualization\n7. Feature engineering\n8. Feature selection\n9. Feature scaling","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Loading packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer # 결측값 대치\nfrom sklearn.preprocessing import PolynomialFeatures # 다항회귀\nfrom sklearn.preprocessing import StandardScaler # 데이터 전처리 스케일 조정\nfrom sklearn.feature_selection import VarianceThreshold # ?? 기능 선택 알고리즘?\nfrom sklearn.feature_selection import SelectFromModel # 중요도 가중치를 기반으로 기능을 선택하기 위한 메타 변환기\nfrom sklearn.utils import shuffle # 배열 또는 희소 행렬을 섞는다\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100) # 최대 열 수 지정","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:32.779223Z","iopub.execute_input":"2022-02-13T08:47:32.779932Z","iopub.status.idle":"2022-02-13T08:47:34.165245Z","shell.execute_reply.started":"2022-02-13T08:47:32.77984Z","shell.execute_reply":"2022-02-13T08:47:34.164402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from sklearn.preprocessing import Import Imputer가 from sklearn.impute import Simplelmputer로 바뀌었습니다.","metadata":{}},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ntest = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:34.166776Z","iopub.execute_input":"2022-02-13T08:47:34.167Z","iopub.status.idle":"2022-02-13T08:47:44.265018Z","shell.execute_reply.started":"2022-02-13T08:47:34.166974Z","shell.execute_reply":"2022-02-13T08:47:44.263678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data at first sight\n\n데이터에 대한 정보들 :\n- 비슷한 Group에 태그된 Feature들은 비슷한 이름을 가지고 있습니다.(예를 들어, ind, reg, car, calc)\n- bin 이라는 접미사를 가진 Feature는 Binary feature임을 나타내고, cat이라는 접미사를 가진 Feature는 Categorical feature임을 나타냅니다.\n- 이외의 Feature들은 Continious 혹은 Ordinal feature입니다.\n- 값이 -1인 관측치는 결측값(NaN)을 의미합니다.\n\n중요한 정보에 대해 파악을 했습니다!\n데이터의 전체적인 모습을 확인하기 위하여 앞부분(head)과 뒷부분(tail)을 먼저 확인해보도록 합니다.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:44.266191Z","iopub.execute_input":"2022-02-13T08:47:44.266405Z","iopub.status.idle":"2022-02-13T08:47:44.321099Z","shell.execute_reply.started":"2022-02-13T08:47:44.266379Z","shell.execute_reply":"2022-02-13T08:47:44.320056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:44.323958Z","iopub.execute_input":"2022-02-13T08:47:44.324288Z","iopub.status.idle":"2022-02-13T08:47:44.37119Z","shell.execute_reply.started":"2022-02-13T08:47:44.324246Z","shell.execute_reply":"2022-02-13T08:47:44.369857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"우리는 다음과 같은 정보를 확인할 수 있습니다.\n\n- binary variables\n- 값이 정수로 이루어진 categorical variables\n- 값이 정수 혹은 소수로 이루어진 other variables\n- 관측값이 -1인 값은 결측치(NaN)를 나타냅니다.\n- Target Variables와 ID\n\nshape을 활용하여 전체 데이터의 행과 열 개수를 확인합니다.","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:44.372795Z","iopub.execute_input":"2022-02-13T08:47:44.373039Z","iopub.status.idle":"2022-02-13T08:47:44.379959Z","shell.execute_reply.started":"2022-02-13T08:47:44.373012Z","shell.execute_reply":"2022-02-13T08:47:44.379034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"59개의 variables와 595,212개의 row가 있습니다.\n다음으로 test 데이터도 똑같은 수의 variables가 있는지 확인해봅니다.\n그 전에, training 데이터에 중복값이 있는지 확인합니다.","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates() # row마다 중복데이터 탐색해서 중복되는 행이면 제거\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:44.381307Z","iopub.execute_input":"2022-02-13T08:47:44.381607Z","iopub.status.idle":"2022-02-13T08:47:45.945005Z","shell.execute_reply.started":"2022-02-13T08:47:44.381573Z","shell.execute_reply":"2022-02-13T08:47:45.941832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"중복값이 없습니다.","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:45.948221Z","iopub.execute_input":"2022-02-13T08:47:45.948964Z","iopub.status.idle":"2022-02-13T08:47:45.964811Z","shell.execute_reply.started":"2022-02-13T08:47:45.948868Z","shell.execute_reply":"2022-02-13T08:47:45.963975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training 데이터와 달리 test 데이터에는 58개의 variables가 있습니다.\n하지만 이 variable의 target variable이므로 상관없습니다.\n\n이제는 우리가 가진 variables들의 타입을 조사해보도록 하겠습니다.\n\n추후에 우리는 14개의 Categorical variables를 더미화 시킬 것입니다. 접미사로 bin이 붙은 variables는 이미 0과 1로 구성되어 있으므로 따로 더미화시킬 필요가 없습니다.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:45.96572Z","iopub.execute_input":"2022-02-13T08:47:45.965953Z","iopub.status.idle":"2022-02-13T08:47:46.067311Z","shell.execute_reply.started":"2022-02-13T08:47:45.965917Z","shell.execute_reply":"2022-02-13T08:47:46.066449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"info()메소드를 사용함으로써 우리는 다시 한 번 데이터들의 타입이 integer 혹은 floated 임을 확인했습니다. info() 메소드 상으로는 결측값이 없는 것으로 확인됩니다. 앞서 언급했듯이 결측값이 -1로 대치되어있기 때문입니다. 우리는 이를 나중에 다루도록 하겠습니다","metadata":{}},{"cell_type":"markdown","source":"## Metadata\n\n데이터 관리를 용이하게 하기 위해서, 우리는 variables의 정보들을 데이터프레임의 형태로 저장하고자 합니다. 이 메타 데이터들은 분석에 필요한 특정한 variables를 선택할 때, 모델링을 할 때, 모델링을 할 때 등에 도움이 될 것입니다.\n\n구체적으로 우리가 저장해야 할 것들은 다음과 같습니다:\n- role : input, ID, target\n- level : nominal, interval, ordinal, binary\n- keep : True or False\n- dtype : int, float, str","metadata":{}},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # role을 정의합니다.\n    # target과 id를 지정해준 뒤, 나머지는 모두 input으로 지정합니다.\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    # level 을 정의합니다\n    # target과 bin은 binary, id와 cat은 nominal, 나머지는 데이터 타입에 따라 float과 int로 지정합니다.\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == 'float64':\n        level = 'interval'\n    elif train[f].dtype == 'int64':\n        level = 'ordinal'\n    \n    # 아이디를 제외한 모든 variables를 True로 지정합니다\n    keep = True\n    if f == 'id':\n        keep = False\n        \n    # 데이터 타입을 지정합니다.\n    dtype = train[f].dtype\n    \n    # 모든 variable의 메타 데이터를 담은 딕셔너리를 생성합니다\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.068438Z","iopub.execute_input":"2022-02-13T08:47:46.068694Z","iopub.status.idle":"2022-02-13T08:47:46.08165Z","shell.execute_reply.started":"2022-02-13T08:47:46.068663Z","shell.execute_reply":"2022-02-13T08:47:46.080729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"원본에서는 메타 데이터의 level을 구분할때 train[f].dtype == float과 같은 형식으로 코드가 작성됐습니다. 제 코드에서는 'float64'로 수정이 되어 있는데, 기존의 코드대로 진행하면 dtype부분에서 원하는 대로 데이터가 구분되지 않습니다. 데이터 형식 뒤에 64도 반드시 입력해줘야 합니다.","metadata":{}},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.085248Z","iopub.execute_input":"2022-02-13T08:47:46.085721Z","iopub.status.idle":"2022-02-13T08:47:46.113229Z","shell.execute_reply.started":"2022-02-13T08:47:46.085677Z","shell.execute_reply":"2022-02-13T08:47:46.112533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"예시로 level이 nominal인 데이터의 인덱스를 추출해봅니다","metadata":{}},{"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.114507Z","iopub.execute_input":"2022-02-13T08:47:46.115464Z","iopub.status.idle":"2022-02-13T08:47:46.125163Z","shell.execute_reply.started":"2022-02-13T08:47:46.11542Z","shell.execute_reply":"2022-02-13T08:47:46.124624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"role과 level에 따른 target의 수를 아래를 통해 확인해봅니다.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.126649Z","iopub.execute_input":"2022-02-13T08:47:46.127336Z","iopub.status.idle":"2022-02-13T08:47:46.151075Z","shell.execute_reply.started":"2022-02-13T08:47:46.127284Z","shell.execute_reply":"2022-02-13T08:47:46.150316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive statistics\n\n이제 데이터프레임에 describe메소드를 사용하여 기술통계량을 살펴보도록 합니다. 그러나 describe에 메소드는 categorical, id variable의 기술통계량은 계산해주지 않습니다. 따라서 추후에 categorical variables를 살펴보도록 합니다.\n\n메타 파일을 통하여 손쉽게 기술통계량을 계산할 수 있습니다.","metadata":{}},{"cell_type":"markdown","source":"### Interval variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.152827Z","iopub.execute_input":"2022-02-13T08:47:46.153922Z","iopub.status.idle":"2022-02-13T08:47:46.467597Z","shell.execute_reply.started":"2022-02-13T08:47:46.153875Z","shell.execute_reply":"2022-02-13T08:47:46.466584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### reg variables\n\n- 오직 ps_reg_03 만이 결측값을 가지고 있습니다\n- variables 사이의 범위 (min to max)를 고려했을 때, 우리는 추후에 스케일링(예를 들어 StandardScaler)을 적용할 수도 있습니다. 하지만 우리가 사용하고자 하는 분류기에 따라 다를 것입니다.\n\n#### car variables\n\n- ps_car_12와 ps_car_14가 결측치를 가지고 있습니다.\n- 이 variables 역시, 스케일링이 필요해보입니다.\n\n#### calc variables\n\n- 결측치가 없습니다\n- 이 variables는 최대값이 0.9인것으로 보아 일종의 비율인 것 같습니다\n- 모든 3개의 _calc variables_ 은 매우 비슷한 분포를 가지고 있습니다\n\n전반적으로, interval variables들 간의 범위가 상대적으로 좁음을 확인 가능합니다. 아마도 데이터를 익명화시키기 위하여 몇몇 변환 작업(예를 들어 log)가 이미 적용된 것은 아닐까요?\n\n기술통계를 살펴봄으로써 다음과 같은 결과를 얻었습니다.\n\n- Feature 내부에 결측치의 존재 유무\n- min 과 max를 비교함으로써 스케일링의 필요성 판단\n- max값을 살펴봄으로써 변수의 값이 비율인지 판단","metadata":{}},{"cell_type":"markdown","source":"### Ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.469333Z","iopub.execute_input":"2022-02-13T08:47:46.469664Z","iopub.status.idle":"2022-02-13T08:47:46.864465Z","shell.execute_reply.started":"2022-02-13T08:47:46.469624Z","shell.execute_reply":"2022-02-13T08:47:46.863299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 결측치를 가지는 variable은 ps_car_11\n- 다른 범위를 가지는 값들에 대하여 스케일링이 필요해보입니다.","metadata":{}},{"cell_type":"markdown","source":"### Binary variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:46.865659Z","iopub.execute_input":"2022-02-13T08:47:46.865888Z","iopub.status.idle":"2022-02-13T08:47:47.258127Z","shell.execute_reply.started":"2022-02-13T08:47:46.86586Z","shell.execute_reply":"2022-02-13T08:47:47.256351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- train 데이터의 target 평균은 0.0036448로 3.645%입니다. 결과값이 매우 불균형함을 알 수 있습니다.\n- 평균값을 통해 대다수의 variables가 0이라고 결론내릴 수 있습니다","metadata":{}},{"cell_type":"markdown","source":"#### Handling imbalanced classes\n\n위에 언급했듯이 target = 1의 비율이 target = 0 비율보다 매우 적습니다(0.963 VS 0.036). 결과값이 불균형한 모델은 높은 정확도를 가지지만 실제로는 부가적인 value가 추가될 수 있습니다. 이러한 문제를 해결하기 위해서 두 가지 방법을 사용할 수 있습니다\n\n- oversampling records with target = 1\n- undersampling records with target= 0\n\n우리는 큰 training set을 가지고 있으므로, undersampling 을 진행하겠습니다. 비율은 0.9:0.1로 지정합니다. 이를 통해 우리는 매우 불균형한 결과값 데이터는 10% 미만임을 확인 가능하며, 10% 정도로 undersampling을 진행하는 것을 확인할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"desired_apriori=0.10\n\n# target value의 인덱스를 추출합니다.\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# target value의 기존 record 수를 구합니다\nnb_0 = len(train.loc[idx_0]) # 573518개\nnb_1 = len(train.loc[idx_1]) # 21694개\n\n# undersampling 비율을 계산하고 target == 0인 record수를 계산합니다\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\n#((1-0.1)*573518) / (0.1*21694)\n# undersampling_Rate 계산 공식을 암기해둡시다\nundersampled_nb_0 = int(undersampling_rate*nb_0)\n\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# shuffle을 활용하여 undersampling 된 개수만큼의 samples를 가지는 nb=0을 무작위로 추출합니다\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# 추출한 인덱스와 기존의 idx_1을 활용하여 리스트를 만듭니다\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Undersample된 데이터 프레임을 돌려받습니다\ntrain = train.loc[idx_list].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:47.259863Z","iopub.execute_input":"2022-02-13T08:47:47.260421Z","iopub.status.idle":"2022-02-13T08:47:47.962001Z","shell.execute_reply.started":"2022-02-13T08:47:47.260374Z","shell.execute_reply":"2022-02-13T08:47:47.961079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Quality Checks","metadata":{}},{"cell_type":"markdown","source":"#### Checking missing values\n결측값은 -1로 나타내지고 있습니다","metadata":{}},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0 :\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \n        print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:47.963439Z","iopub.execute_input":"2022-02-13T08:47:47.963751Z","iopub.status.idle":"2022-02-13T08:47:48.11873Z","shell.execute_reply.started":"2022-02-13T08:47:47.963717Z","shell.execute_reply":"2022-02-13T08:47:48.117585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ps_car_03_cat과 ps_car_05_cat는 높은 결측치 비율을 가지고 있습니다(68.39%, 44.26%). 따라서 삭제해주도록 합니다\n- 결측값이 있는 다른 Cat Variables는 결측값을 -1 그대로 둘 수 있습니다\n- ps_reg_03(continuous)은 18%의 결측값을 지니고 있습니다. 평균값으로 대치해줍니다\n- ps_car_11(ordinal)은 5개의 결측값을 지니고 있습니다. ordinal의 형태이므로 평균값으로 대치하면 안됩니다. 최빈값으로 대치해줍니다\n- ps_car_12(continuous)은 1개의 결측값을 지니고 있습니다. 평균값으로 대치해줍니다\n- ps_car_14(continuous)은 7%의 결측값을 지니고 있습니다. 평균값으로 대치해줍니다","metadata":{}},{"cell_type":"code","source":"# 너무 많은 결측값을 지닌 feature들을 제거합니다(68.4%, 44.3%)\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop), 'keep'] = False # 메타데이터를 업데이트해줍니다\n\n# 결측값을 Imputer를 활용하여 변환해줍니다\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.120448Z","iopub.execute_input":"2022-02-13T08:47:48.120688Z","iopub.status.idle":"2022-02-13T08:47:48.240711Z","shell.execute_reply.started":"2022-02-13T08:47:48.120661Z","shell.execute_reply":"2022-02-13T08:47:48.239761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking the cardinality of the categorical variables\n\n카디날리티는 전체 행에 대한 특정 컬럼의 중복 수치를 나타내는 지표입니다.\n중복도가 높으면 카디날리티가 낮으며, 중복도가 낮으면 카디날리티가 높습니다.\n카디날리티는 상대적인 개념으로 이해해야 합니다\n\n따라서 카디날리티는 variable내에서 다른 value의 개수를 말합니다. 우리는 추후 categorical variables를 더미화시킬 것인데, variables내에 다른 value들이 얼마나 많은지 체크해봐야 합니다. Value들이 많을 경우, 수 많은 더미 변수들이 만들어질 수 있기 때문입니다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.242036Z","iopub.execute_input":"2022-02-13T08:47:48.242261Z","iopub.status.idle":"2022-02-13T08:47:48.273381Z","shell.execute_reply.started":"2022-02-13T08:47:48.242234Z","shell.execute_reply":"2022-02-13T08:47:48.272253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"합리적이긴 하지만, ps_car_11_cat는 104개로 매우 많은 value를 가지고 있습니다\nEDIT : 최초 작성자분은 104개의 value에 대해 가공을 하여 데이터 손실이 있었던 것으로 보입니다. 이후 최초 작성자분은 Oliver의 커널을 활용한 방법을 사용했습니다","metadata":{}},{"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrelier\n# Code : https://www.kaggle.com/ogrelier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None,\n                 tst_series=None,\n                 target=None,\n                 min_samples_leaf=1,\n                 smoothing=1,\n                 noise_level=0):\n\n    \"\"\" \n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf(int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior\n    \"\"\"\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # compute smoothing\n    smoothing = 1/(1+np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1-smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    fr_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name:'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name+'_mean').fillna(prior)\n    #pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index':target.name, target.name:'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name+'_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.275012Z","iopub.execute_input":"2022-02-13T08:47:48.275323Z","iopub.status.idle":"2022-02-13T08:47:48.292198Z","shell.execute_reply.started":"2022-02-13T08:47:48.275283Z","shell.execute_reply":"2022-02-13T08:47:48.291176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이거 한글 번역하신 필자분의 말씀..\n\n상기 과정이 어떻게 이루어지는 이해하기 어려웠다. 기본적으로 너무 value가 많은 categorical variables를 가공하는 과정임은 이해하고 있을 것이다. 따라서 함수의 과정을 하나하나 따라가보도록 하겠다. 이해가 안되었던 분이라면 함께 하시면 좋을 것 같다!","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.293398Z","iopub.execute_input":"2022-02-13T08:47:48.293806Z","iopub.status.idle":"2022-02-13T08:47:48.310974Z","shell.execute_reply.started":"2022-02-13T08:47:48.293765Z","shell.execute_reply":"2022-02-13T08:47:48.309698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"노이즈를 일으키는 함수를 정의함\n\n시리즈값과 노이즈 레벨을 변수로 받아서, 시리즈(1 + 노이즈 레벨 표준정규분포로부터 샘플링된 난수)를 되돌려줍니다.\n\ntarget encode 함수는 assert부터 진행","metadata":{}},{"cell_type":"code","source":"assert len(train[\"ps_car_11_cat\"]) == len(train['target'])\nassert train[\"ps_car_11_cat\"].name == test[\"ps_car_11_cat\"].name","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.313058Z","iopub.execute_input":"2022-02-13T08:47:48.313496Z","iopub.status.idle":"2022-02-13T08:47:48.325877Z","shell.execute_reply.started":"2022-02-13T08:47:48.313465Z","shell.execute_reply":"2022-02-13T08:47:48.32489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"trn_series는 trin['ps_car_11_cat']이고, target은 train['target']이다.\n두 시리즈의 길이가 같은지 확인하고, train['ps_car_11_cat']의 이름이 같은지도 확인해준다","metadata":{}},{"cell_type":"code","source":"temp = pd.concat([train[\"ps_car_11_cat\"], train['target']], axis=1)\nprint(temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.327518Z","iopub.execute_input":"2022-02-13T08:47:48.328444Z","iopub.status.idle":"2022-02-13T08:47:48.347357Z","shell.execute_reply.started":"2022-02-13T08:47:48.327761Z","shell.execute_reply":"2022-02-13T08:47:48.34568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"두 시리즈를 열을 기준으로 concat한 뒤, temp라는 이름의 변수로 저장해주자","metadata":{}},{"cell_type":"code","source":"averages = temp.groupby(train['ps_car_11_cat'].name)[train['target'].name].agg(['mean', 'count'])\nprint(averages)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.348966Z","iopub.execute_input":"2022-02-13T08:47:48.349246Z","iopub.status.idle":"2022-02-13T08:47:48.375981Z","shell.execute_reply.started":"2022-02-13T08:47:48.34921Z","shell.execute_reply":"2022-02-13T08:47:48.375363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"target열의 값들을 train['ps_car_11_cat']을 기준으로 그룹화한뒤 mean함수와 count함수를 적용한 값을 averages변수에 저장한다\n\naverage함수는 각 value별 target평균과 횟수 정보를 담고 있다","metadata":{}},{"cell_type":"code","source":"smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - 100) / 10))\nprint(smoothing)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.376989Z","iopub.execute_input":"2022-02-13T08:47:48.377265Z","iopub.status.idle":"2022-02-13T08:47:48.384868Z","shell.execute_reply.started":"2022-02-13T08:47:48.377237Z","shell.execute_reply":"2022-02-13T08:47:48.38424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"정확히 smoothing이 무슨 작업을 하는지는 좀 더 공부해야한다","metadata":{}},{"cell_type":"code","source":"prior = train['target'].mean()\n\naverages[train['target'].name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\nprint(averages)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.385766Z","iopub.execute_input":"2022-02-13T08:47:48.385989Z","iopub.status.idle":"2022-02-13T08:47:48.404616Z","shell.execute_reply.started":"2022-02-13T08:47:48.385962Z","shell.execute_reply":"2022-02-13T08:47:48.403666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"prior값은 train['target']의 평균값으로 한다. 앞서 undersampling을 통해 0과 1의 비율을 9:1로 맞추었기 때문에 prior값은 0.1이 된다\n\n이후 averages에 target이라는 이름을 가진 열을 추가해준다. 값은 smoothing을 활용하여 변환된다","metadata":{}},{"cell_type":"code","source":"averages.drop(['mean', 'count'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.405757Z","iopub.execute_input":"2022-02-13T08:47:48.406403Z","iopub.status.idle":"2022-02-13T08:47:48.428362Z","shell.execute_reply.started":"2022-02-13T08:47:48.406372Z","shell.execute_reply":"2022-02-13T08:47:48.427622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"필요한 값은 smoothing한 값뿐인 것 같다. drop을 활용해서 mean과 count를 빼준다","metadata":{}},{"cell_type":"code","source":"ft_trn_series = pd.merge(\n        train['ps_car_11_cat'].to_frame(train['ps_car_11_cat'].name),\n        averages.reset_index().rename(columns={'index': train['target'].name, train['target'].name: 'average'}),\n        on=train['ps_car_11_cat'].name,\n        how='left')['average'].rename(train['ps_car_11_cat'].name + '_mean').fillna(prior)\n\nprint(ft_trn_series)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.432875Z","iopub.execute_input":"2022-02-13T08:47:48.433496Z","iopub.status.idle":"2022-02-13T08:47:48.477953Z","shell.execute_reply.started":"2022-02-13T08:47:48.433462Z","shell.execute_reply":"2022-02-13T08:47:48.477095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. ps_car_11_cat시리즈를 to_frame으로 데이터 프레임으로 가져온다\n2. averages의 인덱스를 초기화하고, 인덱스 값의 명칭을 target, target이었던 열 이름을 average로 바꿔준다\n3. on 값과 how 값을 지정해주고 merge해준다\n4. 시리즈의 명칭을 ps_car_11_cat_mean으로 rename해주고 결측값은 prior값으로 대치해준다","metadata":{}},{"cell_type":"code","source":"ft_trn_series.index = train[\"ps_car_11_cat\"].index","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.478995Z","iopub.execute_input":"2022-02-13T08:47:48.479446Z","iopub.status.idle":"2022-02-13T08:47:48.483183Z","shell.execute_reply.started":"2022-02-13T08:47:48.479415Z","shell.execute_reply":"2022-02-13T08:47:48.482392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"새롭게 만든 시리즈의 인덱스를 기존 트레이닝 데이터 시리즈 인덱스와 맞춰줍니다","metadata":{}},{"cell_type":"code","source":"ft_tst_series = pd.merge(test['ps_car_11_cat'].to_frame(test['ps_car_11_cat'].name),\n                              averages.reset_index().rename(columns={'index' : 'target', 'target' : 'averages'}),\n                              on=test['ps_car_11_cat'].name,\n                              how='left')['averages'].rename(train['ps_car_11_cat'].name + '_mean').fillna(prior)\n\nft_tst_series.index = test[\"ps_car_11_cat\"].index","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.484461Z","iopub.execute_input":"2022-02-13T08:47:48.484796Z","iopub.status.idle":"2022-02-13T08:47:48.613372Z","shell.execute_reply.started":"2022-02-13T08:47:48.484768Z","shell.execute_reply":"2022-02-13T08:47:48.612698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"앞서 train 데이터에서 진행했던 작업을 test데이터에서도 그대로 진행한다","metadata":{}},{"cell_type":"code","source":"add_noise(ft_trn_series, 0.01), add_noise(ft_tst_series, 0.01)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.614449Z","iopub.execute_input":"2022-02-13T08:47:48.614725Z","iopub.status.idle":"2022-02-13T08:47:48.663062Z","shell.execute_reply.started":"2022-02-13T08:47:48.614695Z","shell.execute_reply":"2022-02-13T08:47:48.662232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"처음에 정의했던 add_noise함수를 활용해서 노이즈를 일으킨 값들을 반환받는다\n함수 진행과정은 다음과 같음\n\n1. noise를 생성할 add_noise함수 정의\n2. train데이터와 target데이터의 len이 같은지, test데이터와 train데이터의 이름이 같은지 확인\n3. train시리즈와 target시리즈를 concat\n4. value별 mean과 count계산해서 Averages로 저장\n5. Smoothing을 계산\n6. prior를 target 데이터의 평균값으로 정의\n7. 앞서 진행한 value별 평균에 smoothing을 진행하고 필요없어진 mean과 count제거\n8. Averages의 값으로 새로운 시리즈(trn/test_cat_mean)정의\n9. 최초 정의한 add_noise를 적용한 시리즈 반환","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:07:47.873608Z","iopub.execute_input":"2022-02-11T16:07:47.873947Z","iopub.status.idle":"2022-02-11T16:07:47.882904Z","shell.execute_reply.started":"2022-02-11T16:07:47.873914Z","shell.execute_reply":"2022-02-11T16:07:47.881558Z"}}},{"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:48.664105Z","iopub.execute_input":"2022-02-13T08:47:48.664689Z","iopub.status.idle":"2022-02-13T08:47:49.17515Z","shell.execute_reply.started":"2022-02-13T08:47:48.66465Z","shell.execute_reply":"2022-02-13T08:47:49.172654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Visualization","metadata":{}},{"cell_type":"markdown","source":"#### Categorical variables\ntarget값이 1인 categorical variables와 customers의 비율을 살펴보도록 합시다","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v :\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f], as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:49.178289Z","iopub.execute_input":"2022-02-13T08:47:49.179025Z","iopub.status.idle":"2022-02-13T08:47:51.879391Z","shell.execute_reply.started":"2022-02-13T08:47:49.178893Z","shell.execute_reply":"2022-02-13T08:47:51.878373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"막대 그래프들을 통해 결측값이 있는 variables들을 확인할 수 있습니다. 앞서 결측값들을 치환했는데, categorical variables들은 따로 치환을 하지 않았습니다. 최빈값으로 대체하는 것ㅅ보다 분리된 categorical value로서 결측값을 보는 것이 더 좋은 방법일 수 있습니다.\n\n결측값을 가지고 있는 customer들이 다른 value들에 비해 훨씬 높은 target 평균을 가지고 있기 때문입니다","metadata":{}},{"cell_type":"markdown","source":"#### Interval variables\n\ninterval variables의 상관관계를 확인하고자 합니다. heatmap은 variables간의 상관관계를 확인하는데 매우 효율적입니다. 하기 코드는 an example by Michael Waskom에 기반하고 있습니다","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n   \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:51.881288Z","iopub.execute_input":"2022-02-13T08:47:51.881623Z","iopub.status.idle":"2022-02-13T08:47:52.746474Z","shell.execute_reply.started":"2022-02-13T08:47:51.881581Z","shell.execute_reply":"2022-02-13T08:47:52.745623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"강한 상관관계를 가지고 있는 variables는 다음과 같다\n\n- ps_reg_02 and ps_reg_03(0.7)\n- ps_car_12 and ps_car13(0.67)\n- ps_car_12 and ps_car14(0.58)\n- ps_car_13 and ps_car15(0.67)\n\nSeaborn의 pair plot을 사용하면 variables들의 선형관계를 손쉽게 시각화할수있습니다. 하지만 히트맵이 상관관계가 있는 variables들의 관계들을 시각화해주고 있기 때문에, 우리는 높은 상관관계를 보이는 variables들을 분리해서 보고자 합니다\n\nnote프로세스의 속도를 높이기 위해 train데이터의 sample을 사용합니다","metadata":{}},{"cell_type":"code","source":"s = train.sample(frac=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:52.74798Z","iopub.execute_input":"2022-02-13T08:47:52.748372Z","iopub.status.idle":"2022-02-13T08:47:52.780797Z","shell.execute_reply.started":"2022-02-13T08:47:52.748342Z","shell.execute_reply":"2022-02-13T08:47:52.779981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train데이터에서 10%의 데이터를 샘플링합니다\n\n- train.shape -> (216940, 57)\n- s.shape -> (21694, 57)\n\n#### ps_reg_02 and ps_reg_03(0.7)\n\n회귀선이 보여주듯이, 두 variables들 간에는 선형 상관관계를 살펴볼 수 있습니다. hue 파라미터를 통해 target=0과 target=1에 대한 회귀선이 동일함을 알 수 있습니다","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:52.782311Z","iopub.execute_input":"2022-02-13T08:47:52.782939Z","iopub.status.idle":"2022-02-13T08:47:55.40474Z","shell.execute_reply.started":"2022-02-13T08:47:52.782885Z","shell.execute_reply":"2022-02-13T08:47:55.403629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ps_car_12 and ps_car_13(0.67)","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:55.406348Z","iopub.execute_input":"2022-02-13T08:47:55.406718Z","iopub.status.idle":"2022-02-13T08:47:57.293465Z","shell.execute_reply.started":"2022-02-13T08:47:55.406676Z","shell.execute_reply":"2022-02-13T08:47:57.292359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ps_car_12 and ps_car_14(0.58)","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha': 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:57.295038Z","iopub.execute_input":"2022-02-13T08:47:57.295365Z","iopub.status.idle":"2022-02-13T08:47:59.21684Z","shell.execute_reply.started":"2022-02-13T08:47:57.295324Z","shell.execute_reply":"2022-02-13T08:47:59.2159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ps_car_13 and ps_car_15(0.67)","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:47:59.218102Z","iopub.execute_input":"2022-02-13T08:47:59.218349Z","iopub.status.idle":"2022-02-13T08:48:01.043594Z","shell.execute_reply.started":"2022-02-13T08:47:59.218319Z","shell.execute_reply":"2022-02-13T08:48:01.042906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제는 어떤 correlated variables를 유지할지 결정해야 합니다. 이를 위하여 우리는 Principal Component Analysis(PCA), 주성분 분석을 실시하여 variables의 dimensions를 줄일 수 있습니다. 하지만 correlated variables의 수가 적은만큼, 우리의 모델이 heavy-lifting을 하도록 해야합니다.","metadata":{}},{"cell_type":"markdown","source":"#### Checking the correlations between ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:48:01.044537Z","iopub.execute_input":"2022-02-13T08:48:01.045241Z","iopub.status.idle":"2022-02-13T08:48:02.694376Z","shell.execute_reply.started":"2022-02-13T08:48:01.045204Z","shell.execute_reply":"2022-02-13T08:48:02.693747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ordinal variables는 큰 상관관계를 가지고 있지 않은 것으로 보입니다. 반면에 target값으로 그룹화할때 분포가 어떻게 될지 확인할 수 있습니다","metadata":{}},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"markdown","source":"#### Creating dummy variables\n\ncategorical variables는 어떤 순서나 검증이 담겨있지 않습니다. 예를 들어서 카테고리 1보다 2배의 값을 가지고 있지 않습니다. 이 문제는 더미 데이터를 만들어줌으로써 해결할 수 있습니다. 첫번째 dummy variable의 정보는 원래 variables의 범주에 대해 생성된 다른 dummy variable에서 파생될 수 있으므로 삭제해주도록 합니다","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:48:02.695507Z","iopub.execute_input":"2022-02-13T08:48:02.695846Z","iopub.status.idle":"2022-02-13T08:48:02.974502Z","shell.execute_reply.started":"2022-02-13T08:48:02.695818Z","shell.execute_reply":"2022-02-13T08:48:02.973618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dummy variables는 training 데이터 세트에 52개의 variables 를 추가했습니다","metadata":{}},{"cell_type":"markdown","source":"#### Creating interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True) # poly처리가 되지 않은 기존의 열들을 삭제한다\n\n# interactions와 train 데이터를 합쳐줍니다\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:48:02.97577Z","iopub.execute_input":"2022-02-13T08:48:02.97601Z","iopub.status.idle":"2022-02-13T08:48:03.547351Z","shell.execute_reply.started":"2022-02-13T08:48:02.97598Z","shell.execute_reply":"2022-02-13T08:48:03.546489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PolynomialFeatures는 다항함수 변환을 진행을 도와주는 클래스입니다. 상기 코드의 경우 degree를 2로 설정했으니 2차항 변수로 만들어주는 것입니다\n\n이를 통해 train 데이터에 interaction variables를 추가할 수 있습니다. get_feature_names 메소드 덕분에 열 이름을 할당할 수 있습니다.","metadata":{}},{"cell_type":"markdown","source":"### Feature selection","metadata":{}},{"cell_type":"markdown","source":"#### Removing features with low or zero variance","metadata":{}},{"cell_type":"markdown","source":"개인적으로 작성자는 분류기의 알고리즘이 유지할 feature를 선택하는 것을 선호한다고 합니다. 하지만 우리 스스로 할 수 있는 일도 있습니다. 분산이 0이거나 아주 적은 features들을 제거하는 것입니다.\n\n이를 위해 사이킷런 VarianceThreshold라는 메소드를 사용할 수 있습니다. 기본적으로 이 메소드는 분산 값이 0인 features들을 제거해줍니다.\n\n하지만 저희는 이전 단계에서 이미 분산이 0인 features가 없음을 확인했기 때문에, 우리는 1%미만의 분산이 있는 feature들을 제거해주고자 합니다. 이를 통해 우리는 31개의 variables를 제거하게 됩니다","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables(id랑 타겟 변수를 제외하고 fit)\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements(불리언 배열 요소를 전환)\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:48:03.548485Z","iopub.execute_input":"2022-02-13T08:48:03.548742Z","iopub.status.idle":"2022-02-13T08:48:04.313494Z","shell.execute_reply.started":"2022-02-13T08:48:03.548711Z","shell.execute_reply":"2022-02-13T08:48:04.312616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"만약 우리가 분산에 기반하여 선택을 진행한다면 많은 variables들을 잃게 될 것입니다. 하지만 우리는 많이 variables를 가지고 있지 않기 때문에, 분류기가 직접 선택하도록 합니다. variables가 더 많은 데이터셋이라면 처리 시간을 줄여줄 수 있을 것입니다.\n\n사이킷런은 [feature selection methods]를 제공합니다. 이 메소드 중 하나가 'SelectFromModel'인데, 다른 분류기에서 최상의 feature를 선택하고 기능을 계속할 수 있도록 합니다. 아래를 통해 랜덤 포레스트를 어떻게 사용하는지 확인해보도록 합니다.","metadata":{}},{"cell_type":"markdown","source":"#### Selecting features with a Random Forest and SelectFromModel\n\n우리는 랜덤 포레스트의 feature importances에 따라 feature 선택의 기준을 삼습니다.\nSelectFromModel을 통하여 유지할 variables의 숫자를 구체화할 수 있습니다. feature의 중요도에 대한 임곗값을 수동으로 설정할 수 있지만, 우리는 단순히 50% 이상의 최적의 variables를 선택해보도록 합시다.","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:48:04.31489Z","iopub.execute_input":"2022-02-13T08:48:04.315274Z","iopub.status.idle":"2022-02-13T08:59:18.711213Z","shell.execute_reply.started":"2022-02-13T08:48:04.315221Z","shell.execute_reply":"2022-02-13T08:59:18.710251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True) # 중앙값으로 지정\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:59:18.713233Z","iopub.execute_input":"2022-02-13T08:59:18.713579Z","iopub.status.idle":"2022-02-13T08:59:19.778334Z","shell.execute_reply.started":"2022-02-13T08:59:18.713515Z","shell.execute_reply":"2022-02-13T08:59:19.777554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[selected_vars + ['target']]","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:59:19.780632Z","iopub.execute_input":"2022-02-13T08:59:19.780849Z","iopub.status.idle":"2022-02-13T08:59:19.841784Z","shell.execute_reply.started":"2022-02-13T08:59:19.780823Z","shell.execute_reply":"2022-02-13T08:59:19.840839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature scaling\n\n이전에 언급했듯이, 우리는 train데이터에 정규화를 진행할 수 있습니다. 몇몇 분류기에서는 더 나은 결과를 가져올 수 있을 것입니다.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T08:59:19.843801Z","iopub.execute_input":"2022-02-13T08:59:19.84411Z","iopub.status.idle":"2022-02-13T08:59:20.405441Z","shell.execute_reply.started":"2022-02-13T08:59:19.84407Z","shell.execute_reply":"2022-02-13T08:59:20.40461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{}}]}